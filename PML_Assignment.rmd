---
title: "Practical Machine Learning - Prediction Assignment Writeup"
author: "Cem Ilgun"
date: "April 30, 2016"
output: html_document
---

```{r setup, include=FALSE, message=FALSE, warning=FALSE, echo=FALSE, eval=TRUE}
knitr::opts_chunk$set(echo = TRUE)
library(sjPlot)
library(caret)
library(knitr)
library(captioner)
library(doParallel)
library(randomForest)
registerDoParallel(cores = 3)

opts_chunk$set(message=FALSE, warning=FALSE, 
               echo=FALSE, cache=TRUE,
               fig.path="images/")
fig_num <- captioner(prefix = "Figure")
table_num <- captioner(prefix = "Table")
appendix_num <- captioner(prefix = "Appendix")
#load("data/assignment-data.rda")
```

## Introduction
For the "Practical Machine Learning" course at Coursera, the class was given
a dataset from a Human Activity Recognition (HAR) study that tries to assess 
the quality of an activity (defined as <q>... *the adherence of the execution 
of an activity to its specification* ...</q>), namely a weight lifting exercise,
using data from sensors attached to the individuals and their equipment. 
	
<aside>
For more details see "[The Weight Lifting Exercises Dataset](http://groupware.les.inf.puc-rio.br/har#weight_lifting_exercises)"
</aside>
In contrast to other HAR studies, this one[^velloso]
does not attempt to distinguish *what* activity is being done, but 
rather to assess *how well* is the activity being performed.

[^velloso]: Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. [Qualitative Activity Recognition of Weight Lifting Exercises](http://groupware.les.inf.puc-rio.br/public/papers/2013.Velloso.QAR-WLE.pdf). Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013. 

<aside>
**`r fig_num("body sensors", "Location of body sensors")`**[^sensors]
![Location of body sensors](images/on-body-sensing-schema.png)
</aside>

[^sensors]: Image obtained from http://groupware.les.inf.puc-rio.br/har#weight_lifting_exercises
The aforementioned study used sensors that <q>... *provide three-axes
acceleration, gyroscope and magnetometer data* ...</q>, with a Bluetooth
module that allowed experimental data capture. These sensors
were attached (see *`r fig_num("body sensors", display="cite")`*), to 
<q>... *six male participants aged between 20-28 years* ...</q> 
who performed one set of ten repetitions
of the Unilateral Dumbbell Biceps Curl with a 1.25kg (light) dumbbell, 
in five different manners (one correct and four incorrect):

- Exactly according to the specification (Class A)
- Throwing the elbows to the front (Class B)
- Lifting the dumbbell only halfway (Class C)
- Lowering the dumbbell only halfway (Class D)
- Throwing the hips to the front (Class E)

## Getting and cleaning the data

```{r get-clean}
getDataFile <- function(url) {
  outfile <- basename(url)
  if (!file.exists(paste0(outfile, ".gz"))) {
    download.file(url = url, destfile = outfile, method = "curl")
    system(paste0("gzip -9 ", outfile))
  }
  read.csv(paste0(outfile, ".gz"), stringsAsFactors = FALSE)
}

traningFileURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
trainingData <- getDataFile(traningFileURL)

testFileURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
testData <- getDataFile(testFileURL)
```

There were two datasets in CSV format, one to be used for
[training](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-train_data.csv), and another one for [testing](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing_data.csv). The training dataset contained `r nrow(trainingData)` rows and 
`r ncol(trainingData)` columns, including the `classe` variable 
which classified the entry according to the how well the exercise was 
performed (*vide supra*). The testing  dataset has only 
`r nrow(testData)` rows and `r ncol(testData)` columns, and
instead of the `classe` variable there is an `problem_id` column to be
used as an identifier for the prediction results. The latter set,
was to be used for a different part of the assignment dealing 
with specific class prediction.

<aside>
```{r table7cols}
dataFrame <- as.data.frame(sapply(trainingData[,1:7], class))
dataFrame$Type <- dataFrame[,1]
dataFrame[,1] <- NULL
t7c <- sjt.df(dataFrame, describe = FALSE, showRowNames = TRUE,
              no.output = TRUE, hideProgressBar = TRUE, alternateRowColors = TRUE)
```
**`r table_num("train7cols", "First 7 columns of the training dataset")`**
	
`r t7c$knitr`
</aside>

```{r remove7cols}
train_data <- trainingData[, -c(1:7)]
test_data <- testData[, -c(1:7)]
```
	
The first seven columns of the training dataset 
(*`r paste(colnames(trainingData[,1:7]), collapse="*, *")`*)
are not related to the sensor measurements, but rather to the
identity of the person, and the time stamps and capture windows
for the sensor data 
(see `r table_num("train7cols", display="cite")`). 
Because I am trying to produce a predictive model that only relies on 
the quantitative sensor measurements, I decided to remove
these columns. In a similar fashion, the first seven columns of the testing 
dataset were also removed. This operation left me with a total of
`r ncol(train_data)` columns in each data frames.

Thus, the data frame has, for each of the four sensors
(positioned at the arm, forearm, belt, and dumbbell respectively), 
38 different measurements (see `r table_num("sensorcolumns", display="cite")` in 
`r appendix_num("app_sensor_columns", display="cite")`). The problem then is 
to select from these `r ncol(train_data) - 1` variables the ones
relevant to predict a good exercise execution.

The automatic column type assignment of the `read.csv()` R function was not
always correct, in particular because several of the numeric columns contained 
text data coming from sensor reading errors (e.g. "#DIV/0!"). So, I forced all
of the sensor readings to be numeric, and set the `classe` column as a factor.

```{r set-types}
# set classe as a factor
train_data$classe <- factor(train_data$classe)

# convert to numeric all except the last column (classe)
numeric_cols <- colnames(train_data)[-ncol(train_data)]
train_data[numeric_cols] <- sapply(train_data[numeric_cols], as.numeric)
```

As a result of the type assignment some columns contained only `NA` values,
so these were removed from the dataset. Also, by using the
`nearZeroVar()` function of the `caret` package, I eliminated columns that
were considered uninformative (zero or near zero variance predictors).

```{r remove-NAs}
# remove all NA columns
train_data <- train_data[, colSums(is.na(train_data)) < nrow(train_data)]

# remove near zero variace predictors
nzvars <- nearZeroVar(train_data)
train_data <- train_data[, -nzvars]
```


```{r toomanymissing, eval=TRUE}
# Find columns with less than 20% of missing values
na_perc <- colMeans(is.na(train_data))
na_perc_dataFrame <- as.data.frame(table(round(na_perc,2)))
na_perc_dataFrame$Var1 <- 100*as.numeric(as.character(na_perc_dataFrame$Var1))
colnames(na_perc_dataFrame) <- c("Percentage of missing values", "Number of columns")
na_tbl1 <- sjt.df(na_perc_dataFrame, describe = FALSE, showRowNames = FALSE,
                  no.output = TRUE, hideProgressBar = TRUE,
                  alternateRowColors = TRUE)
```

<aside>
**`r table_num("tabmissing", "Number of columns by percentage of missing values")`**

`r na_tbl1$knitr`
</aside>

After that last operation, the training data frame had only 
`r ncol(train_data)` variables including the classification column.
Of these variables, I checked to see how many of them contained
too many missing data values. Initially I set the threshold to 80%, but 
soon found out that there were two cases: columns without any missing 
data, and columns that had about 98% missing data 
(see `r table_num("tabmissing", display="cite")`). 
Trying to impute values in the latter cases could be done, but
is unlikely that it will give anything reasonable or useful as a predictor, 
thus, those `r na_perc_dataFrame[2,2]` columns were also removed.

In the end we will use `r na_perc_dataFrame[1,2] - 1` measurements of 
the *x*, *y*, and *z* axis  components of the acceleration, gyroscope, and magnetometer sensors, 
as well as the *overall acceleration*, *pitch*, *roll* and *yaw* 
(see `r table_num("sensorcolumns2", display="cite")` in
`r appendix_num("app_sensor_columns2", display="cite")`),
to predict whether the exercise was done correctly.

## Generating and validating a Random Forest predictive model
Because the provided testing dataset could not be used to validate
the predictive model, I decided to split the
"training" dataset into one to be used to perform the random forest model 
training (75% of the data), and another to validate it (25% of the data).
The training will also assess the quality of the model using an
"out of bag" (OOB) error estimate using cross-validation.

```{r split-train-data}
filcols <- colnames(train_data)[na_perc <= 0.2]
train_filtered <- train_data[,filcols]
set.seed(9753)
splitTrain <- createDataPartition(train_data$classe, p = 0.75, list = FALSE)
train_set <- train_filtered[splitTrain,]
validation_set <- train_filtered[-splitTrain,]
```

The model training used the standard random forest
(`rf`) algorithm[^rfref] method available in the `caret` package, with
the default parameters and doing a 10-fold cross validation. I used the
`classe` variable as the dependent and `r ncol(train_set) - 1`
sensor variables as predictors.
This model gave an OOB error of 0.6%, which indicates a possible good
classifier.

[^rfref]: [randomForest: Breiman and Cutler's random forests for classification and regression](http://cran.r-project.org/web/packages/randomForest/)

```{r rf-model}
if (!file.exists("model-rf-cv.rda")) {
  set.seed(2468)
  train_ctrl <- trainControl(method = "cv", number = 10)
  mod_rf <- train(classe ~ .,
                  data = train_set,
                  method = "rf",
                  trControl = train_ctrl,
                  do.trace = 500,
                  verbose = FALSE, importance=TRUE)
  save(mod_rf, file = "model-rf-cv.rda")
} else {
  load("model-rf-cv.rda")
}

pred_rf <- predict(mod_rf, newdata = validation_set)
cm_rf <- confusionMatrix(pred_rf, validation_set$classe)
tcm <- sjt.df(as.data.frame(as.matrix(cm_rf)), 
              describe = FALSE, alternateRowColors = TRUE, 
              stringVariable = "", no.output = TRUE,
              hideProgressBar = TRUE)
```

With the reserved validation set, I calculated
the confusion matrix (`r table_num("cm", display="cite")`),
and other relevant statistics using the `confusionMatrix()`
function of the `caret` package. The confusion matrix shows that the
model does a reasonable good job at predicting the exercise quality.
<aside>
**`r table_num("cm", "Confusion Matrix (Predicted vs Reference) for Random Forest model")`**
`r tcm$knitr`
</aside>

Validating the model results in an
accuracy of `r round(cm_rf$overall["Accuracy"], 4)`
(95% confidence interval: 
[`r round(cm_rf$overall["AccuracyLower"], 4)`, 
`r round(cm_rf$overall["AccuracyUpper"], 4)`]). The estimated accuracy
is well above the "no information rate" statistic of 
`r round(cm_rf$overall["AccuracyNull"], 4)`.
The validation results also in a high kappa statistic of 
`r round(cm_rf$overall["Kappa"], 4)`,
which suggest a very good classifier. 
Overall, this model compares well with the 0.9803 accuracy that was 
reported in the original work.
The first 20 model predictors can be seen in `r fig_num("rfimp", display="cite")`,
and the complete list of predictors (ordered by their mean decrease in 
accuracy) is  in `r table_num("trfimp", display="cite")` 
(`r appendix_num("rfimp", display="cite")`)

**`r fig_num("rfimp", "Variable Importance for Random Forest model (first 20 variables)")`**
```{r importance-plot, fig.star=TRUE, fig.width=14}
varImpPlot(mod_rf$finalModel, n.var=20, main="")
``` 

This plot indicates that the measurements of the belt sensor (*roll*, *yaw*,
and *pitch*), the forearm (*pitch*) and the dumbbell (*magnetic component*), 
are the most important for distinguishing whether this
particular exercise is being done correctly or not. This makes sense as the
way the core body moves and the rotation of the forearm, are closely related 
to a correct execution of the biceps curl, and in the case of the metallic
dumbbell the position changes are readily detected by the magnetometer.

## Reproducibility information

The source code for the R Markdown document and other accessory artifacts 
is available at the github repository:
[https://github.com/kirmizi8/PracticalMachineLearning](https://github.com/kirmizi8/PracticalMachineLearning)

```{r}
sessionInfo()
```

## Appendices

### `r appendix_num("app_sensor_columns", "Columns related to the sensors in the original training dataset")`

```{r table-all-sensor-data, eval=TRUE}
allcols <- colnames(trainingData)
allcols <- allcols[order(allcols)]
col_by_sensor <- data.frame(
  arm = allcols[grepl("_arm", allcols)],
  forearm = allcols[grepl("_forearm", allcols)],
  belt = allcols[grepl("_belt", allcols)],
  dumbbell = allcols[grepl("_dumbbell", allcols)],
  stringsAsFactors = FALSE
)

tscols <- sjt.df(col_by_sensor, describe = FALSE,
                 showRowNames = FALSE, no.output = TRUE,
                 hideProgressBar = TRUE, alternateRowColors = TRUE)
```

**`r table_num("sensorcolumns", "Measurement columns by sensor")`**
`r tscols$knitr`

### `r appendix_num("app_sensor_columns2", "Remaining columns related to the sensors")`

```{r table-cleanedup-sensor-data}
filcols <- names(na_perc[na_perc < 0.8])
filcols <- filcols[order(filcols)]
train_data <- train_data[, filcols]
remain_sensor_cols <- data.frame(
  arm = filcols[grepl("_arm", filcols)],
  forearm = filcols[grepl("_forearm", filcols)],
  belt = filcols[grepl("_belt", filcols)],
  dumbbell = filcols[grepl("_dumbbell", filcols)],
  stringsAsFactors = FALSE
)
tscols2 <- sjt.df(remain_sensor_cols, describe = FALSE,
                 showRowNames = FALSE, no.output = TRUE,
                 hideProgressBar = TRUE, alternateRowColors = TRUE)
```

**`r table_num("sensorcolumns2", "Remaining measurement columns by sensor")`**
`r tscols2$knitr`


### `r appendix_num("rfimp", "Random Forest Model - Variable Importance")`

```{r table-model-var-importance}
dataFrame <- as.data.frame(mod_rf$finalModel$importance)
dataFrame <- dataFrame[order(-dataFrame$MeanDecreaseAccuracy),]
tvi_rf <- sjt.df(round(dataFrame, 2), describe = FALSE,
                 showRowNames = TRUE, no.output = TRUE,
                 hideProgressBar = TRUE, alternateRowColors = TRUE)
```

**`r table_num("trfimp", "Variable importance per class and overall")`**
`r tvi_rf$knitr`
